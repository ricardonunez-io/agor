/**
 * Codex Tool Implementation
 *
 * Current capabilities:
 * - ‚úÖ Live execution via OpenAI Codex SDK
 * - ‚ùå Import sessions (deferred - need real session JSONL format)
 * - ‚ùå Session creation (handled via live execution)
 */

import { execSync } from 'node:child_process';
import type { Database } from '../../db/client';
import type { MessagesRepository } from '../../db/repositories/messages';
import type { SessionMCPServerRepository } from '../../db/repositories/session-mcp-servers';
import type { SessionRepository } from '../../db/repositories/sessions';
import type { WorktreeRepository } from '../../db/repositories/worktrees';
import { generateId } from '../../lib/ids';
import {
  type Message,
  type MessageID,
  MessageRole,
  type PermissionMode,
  type Session,
  type SessionID,
  type TaskID,
} from '../../types';
import type { TokenUsage } from '../../utils/pricing';
import { calculateTokenCost } from '../../utils/pricing';
import type {
  CodexSdkResponse,
  NormalizedSdkResponse,
  RawSdkResponse,
} from '../../types/sdk-response';
import type { ITool, StreamingCallbacks, ToolCapabilities } from '../base';
import type { MessagesService, TasksService } from '../claude/claude-tool';
import { DEFAULT_CODEX_MODEL, getCodexContextWindowLimit } from './models';
import { CodexPromptService } from './prompt-service';

interface CodexExecutionResult {
  userMessageId: MessageID;
  assistantMessageIds: MessageID[];
  tokenUsage?: TokenUsage;
  contextWindow?: number;
  contextWindowLimit?: number;
  model?: string;
}

export class CodexTool implements ITool {
  readonly toolType = 'codex' as const;
  readonly name = 'OpenAI Codex';

  private promptService?: CodexPromptService;
  private messagesRepo?: MessagesRepository;
  private sessionsRepo?: SessionRepository;
  private messagesService?: MessagesService;
  private tasksService?: TasksService;

  constructor(
    messagesRepo?: MessagesRepository,
    sessionsRepo?: SessionRepository,
    sessionMCPServerRepo?: SessionMCPServerRepository,
    worktreesRepo?: WorktreeRepository,
    apiKey?: string,
    messagesService?: MessagesService,
    tasksService?: TasksService,
    private db?: Database
  ) {
    this.messagesRepo = messagesRepo;
    this.sessionsRepo = sessionsRepo;
    this.messagesService = messagesService;
    this.tasksService = tasksService;

    if (messagesRepo && sessionsRepo) {
      this.promptService = new CodexPromptService(
        messagesRepo,
        sessionsRepo,
        sessionMCPServerRepo,
        worktreesRepo,
        apiKey,
        db
      );
    }
  }

  getCapabilities(): ToolCapabilities {
    return {
      supportsSessionImport: false, // ‚ùå Deferred until we have real JSONL format
      supportsSessionCreate: false, // ‚ùå Not exposed (handled via executeTask)
      supportsLiveExecution: true, // ‚úÖ Via Codex SDK
      supportsSessionFork: false,
      supportsChildSpawn: false,
      supportsGitState: false, // Agor manages git state
      supportsStreaming: true, // ‚úÖ Via runStreamed()
    };
  }

  async checkInstalled(): Promise<boolean> {
    try {
      // Check if codex CLI is installed
      execSync('which codex', { encoding: 'utf-8' });
      return true;
    } catch {
      return false;
    }
  }

  /**
   * Execute a prompt against a session WITH real-time streaming
   *
   * Creates user message, streams response chunks from Codex, then creates complete assistant messages.
   * Calls streamingCallbacks during message generation for real-time UI updates.
   *
   * @param sessionId - Session to execute prompt in
   * @param prompt - User prompt text
   * @param taskId - Optional task ID for linking messages
   * @param permissionMode - Permission mode for tool execution ('ask' | 'auto' | 'allow-all')
   * @param streamingCallbacks - Optional callbacks for real-time streaming (enables typewriter effect)
   * @returns User message ID and array of assistant message IDs
   */
  async executePromptWithStreaming(
    sessionId: SessionID,
    prompt: string,
    taskId?: TaskID,
    permissionMode?: PermissionMode,
    streamingCallbacks?: StreamingCallbacks
  ): Promise<CodexExecutionResult> {
    if (!this.promptService || !this.messagesRepo) {
      throw new Error('CodexTool not initialized with repositories for live execution');
    }

    if (!this.messagesService) {
      throw new Error('CodexTool not initialized with messagesService for live execution');
    }

    // Get next message index
    const existingMessages = await this.messagesRepo.findBySessionId(sessionId);
    let nextIndex = existingMessages.length;

    // Create user message
    const userMessage = await this.createUserMessage(sessionId, prompt, taskId, nextIndex++);

    // Execute prompt via Codex SDK with streaming
    const assistantMessageIds: MessageID[] = [];
    let capturedThreadId: string | undefined;
    let resolvedModel: string | undefined;
    let currentMessageId: MessageID | null = null;
    let tokenUsage: TokenUsage | undefined;
    let _streamStartTime = Date.now();
    let _firstTokenTime: number | null = null;

    for await (const event of this.promptService.promptSessionStreaming(
      sessionId,
      prompt,
      taskId,
      permissionMode
    )) {
      // Capture resolved model from partial/complete events
      if (!resolvedModel) {
        if (event.type === 'partial') {
          resolvedModel = event.resolvedModel;
        } else if (event.type === 'complete') {
          resolvedModel = event.resolvedModel;
        }
      }

      if (event.type === 'complete' && event.usage) {
        tokenUsage = event.usage;
      }

      // Capture Codex thread ID
      if (!capturedThreadId && event.threadId) {
        capturedThreadId = event.threadId;
        await this.captureThreadId(sessionId, capturedThreadId);
      }

      // Handle partial streaming events (token-level chunks)
      // NOTE: Based on official OpenAI sample, partial events are never emitted by Codex SDK
      // agent_message text arrives all at once in the 'complete' event, not streamed incrementally
      // This code is kept for future compatibility if OpenAI adds true streaming
      if (event.type === 'partial' && event.textChunk) {
        // Start new message if needed
        if (!currentMessageId) {
          currentMessageId = generateId() as MessageID;
          _firstTokenTime = Date.now();

          if (streamingCallbacks) {
            streamingCallbacks.onStreamStart(currentMessageId, {
              session_id: sessionId,
              task_id: taskId,
              role: MessageRole.ASSISTANT,
              timestamp: new Date().toISOString(),
            });
          }
        }

        // Emit chunk immediately
        if (streamingCallbacks) {
          streamingCallbacks.onStreamChunk(currentMessageId, event.textChunk);
        }
      }
      // Handle tool completion (create message immediately for live updates)
      else if (event.type === 'tool_complete') {
        // Create a message for this tool use immediately
        const toolMessageId = generateId() as MessageID;
        const toolContent = [
          {
            type: 'tool_use',
            id: event.toolUse.id,
            name: event.toolUse.name,
            input: event.toolUse.input,
          },
          ...(event.toolUse.output !== undefined || event.toolUse.status
            ? [
                {
                  type: 'tool_result',
                  tool_use_id: event.toolUse.id,
                  content: event.toolUse.output || `[${event.toolUse.status}]`,
                  is_error: event.toolUse.status === 'failed' || event.toolUse.status === 'error',
                },
              ]
            : []),
        ];

        await this.createAssistantMessage(
          sessionId,
          toolMessageId,
          toolContent as Array<{
            type: string;
            text?: string;
            id?: string;
            name?: string;
            input?: Record<string, unknown>;
          }>,
          [
            {
              id: event.toolUse.id,
              name: event.toolUse.name,
              input: event.toolUse.input,
            },
          ],
          taskId,
          nextIndex++,
          resolvedModel
        );
        assistantMessageIds.push(toolMessageId);
      }
      // Handle complete message (save to database)
      else if (event.type === 'complete' && event.content) {
        const usageForMessage = event.usage ?? tokenUsage;
        // Filter out tool_use and tool_result blocks (already saved via tool_complete events)
        // But KEEP text blocks - these contain the response
        const textOnlyContent = event.content.filter(
          block => block.type === 'text' // Only keep text blocks
        );

        // Only create message if there's text content (not just tools)
        if (textOnlyContent.length > 0) {
          // Extract full text for client-side streaming
          const _fullText = textOnlyContent
            .map(block => (block as { text?: string }).text || '')
            .join('');

          // Use existing message ID from streaming (if any) or generate new
          const assistantMessageId = currentMessageId || (generateId() as MessageID);

          // NOTE: Codex SDK doesn't support true streaming for text responses
          // It only emits item.completed with the full text, no item.updated events
          // Text is displayed immediately when the complete event arrives

          // Create complete message in DB (text only, tools already saved)
          await this.createAssistantMessage(
            sessionId,
            assistantMessageId,
            textOnlyContent,
            undefined, // No tool uses in this message (already saved separately)
            taskId,
            nextIndex++,
            resolvedModel,
            usageForMessage
          );
          assistantMessageIds.push(assistantMessageId);

          // Reset for next message
          currentMessageId = null;
        }

        _streamStartTime = Date.now();
        _firstTokenTime = null;
      }
    }

    return {
      userMessageId: userMessage.message_id,
      assistantMessageIds,
      tokenUsage,
      // Codex SDK doesn't provide contextWindow/contextWindowLimit
      contextWindow: undefined,
      contextWindowLimit: undefined,
      model: resolvedModel || DEFAULT_CODEX_MODEL,
    };
  }

  /**
   * Create user message in database
   * @private
   */
  private async createUserMessage(
    sessionId: SessionID,
    prompt: string,
    taskId: TaskID | undefined,
    nextIndex: number
  ): Promise<Message> {
    const userMessage: Message = {
      message_id: generateId() as MessageID,
      session_id: sessionId,
      type: 'user',
      role: MessageRole.USER,
      index: nextIndex,
      timestamp: new Date().toISOString(),
      content_preview: prompt.substring(0, 200),
      content: prompt,
      task_id: taskId,
    };

    await this.messagesService?.create(userMessage);
    return userMessage;
  }

  /**
   * Capture and store Codex thread ID for conversation continuity
   * @private
   */
  private async captureThreadId(sessionId: SessionID, threadId: string): Promise<void> {
    console.log(`üîë Captured Codex thread ID for Agor session ${sessionId}: ${threadId}`);

    if (this.sessionsRepo) {
      await this.sessionsRepo.update(sessionId, { sdk_session_id: threadId });
      console.log(`üíæ Stored Codex thread ID in Agor session`);
    }
  }

  /**
   * Create complete assistant message in database
   * @private
   */
  private async createAssistantMessage(
    sessionId: SessionID,
    messageId: MessageID,
    content: Array<{
      type: string;
      text?: string;
      id?: string;
      name?: string;
      input?: Record<string, unknown>;
    }>,
    toolUses: Array<{ id: string; name: string; input: Record<string, unknown> }> | undefined,
    taskId: TaskID | undefined,
    nextIndex: number,
    resolvedModel?: string,
    tokenUsage?: TokenUsage
  ): Promise<Message> {
    // Extract text content for preview
    const textBlocks = content.filter(b => b.type === 'text').map(b => b.text || '');
    const fullTextContent = textBlocks.join('');
    const contentPreview = fullTextContent.substring(0, 200);

    const message: Message = {
      message_id: messageId,
      session_id: sessionId,
      type: 'assistant',
      role: MessageRole.ASSISTANT,
      index: nextIndex,
      timestamp: new Date().toISOString(),
      content_preview: contentPreview,
      content: content as Message['content'],
      tool_uses: toolUses,
      task_id: taskId,
      metadata: {
        model: resolvedModel || DEFAULT_CODEX_MODEL,
        tokens: {
          input: tokenUsage?.input_tokens ?? 0,
          output: tokenUsage?.output_tokens ?? 0,
        },
      },
    };

    await this.messagesService?.create(message);

    // If task exists, update it with resolved model
    if (taskId && resolvedModel && this.tasksService) {
      await this.tasksService.patch(taskId, { model: resolvedModel });
    }

    return message;
  }

  /**
   * Execute a prompt against a session (non-streaming version)
   *
   * Creates user message, collects response from Codex, creates assistant messages.
   * Returns user message ID and array of assistant message IDs.
   *
   * @param sessionId - Session to execute prompt in
   * @param prompt - User prompt text
   * @param taskId - Optional task ID for linking messages
   * @param permissionMode - Permission mode for tool execution ('ask' | 'auto' | 'allow-all')
   */
  async executePrompt(
    sessionId: SessionID,
    prompt: string,
    taskId?: TaskID,
    permissionMode?: PermissionMode
  ): Promise<CodexExecutionResult> {
    if (!this.promptService || !this.messagesRepo) {
      throw new Error('CodexTool not initialized with repositories for live execution');
    }

    if (!this.messagesService) {
      throw new Error('CodexTool not initialized with messagesService for live execution');
    }

    // Get next message index
    const existingMessages = await this.messagesRepo.findBySessionId(sessionId);
    let nextIndex = existingMessages.length;

    // Create user message
    const userMessage = await this.createUserMessage(sessionId, prompt, taskId, nextIndex++);

    // Execute prompt via Codex SDK
    const assistantMessageIds: MessageID[] = [];
    let capturedThreadId: string | undefined;
    let resolvedModel: string | undefined;
    let tokenUsage: TokenUsage | undefined;
    let contextWindow: number | undefined;
    let contextWindowLimit: number | undefined;

    for await (const event of this.promptService.promptSessionStreaming(
      sessionId,
      prompt,
      taskId,
      permissionMode
    )) {
      // Capture resolved model from partial/complete events
      if (!resolvedModel) {
        if (event.type === 'partial') {
          resolvedModel = event.resolvedModel;
        } else if (event.type === 'complete') {
          resolvedModel = event.resolvedModel;
        }
      }

      if (event.type === 'complete' && event.usage) {
        tokenUsage = event.usage;
      }

      // Capture Codex thread ID
      if (!capturedThreadId && event.threadId) {
        capturedThreadId = event.threadId;
        await this.captureThreadId(sessionId, capturedThreadId);
      }

      // Skip partial and tool events in non-streaming mode
      if (
        event.type === 'partial' ||
        event.type === 'tool_start' ||
        event.type === 'tool_complete'
      ) {
        continue;
      }

      // Handle complete messages only
      if (event.type === 'complete' && event.content) {
        const messageId = generateId() as MessageID;
        const usageForMessage = event.usage ?? tokenUsage;
        await this.createAssistantMessage(
          sessionId,
          messageId,
          event.content,
          event.toolUses,
          taskId,
          nextIndex++,
          resolvedModel,
          usageForMessage
        );
        assistantMessageIds.push(messageId);
      }
    }

    return {
      userMessageId: userMessage.message_id,
      assistantMessageIds,
      tokenUsage,
      // Codex SDK doesn't provide contextWindow/contextWindowLimit
      contextWindow: undefined,
      contextWindowLimit: undefined,
      model: resolvedModel || DEFAULT_CODEX_MODEL,
    };
  }

  /**
   * Stop currently executing task in session
   *
   * Uses a flag-based approach to break the event loop on the next iteration.
   *
   * @param sessionId - Session identifier
   * @param taskId - Optional task ID (not used for Codex, session-level stop)
   * @returns Success status and reason if failed
   */
  async stopTask(
    sessionId: string,
    taskId?: string
  ): Promise<{
    success: boolean;
    partialResult?: Partial<{ taskId: string; status: 'completed' | 'failed' | 'cancelled' }>;
    reason?: string;
  }> {
    if (!this.promptService) {
      return {
        success: false,
        reason: 'CodexTool not initialized with prompt service',
      };
    }

    const result = this.promptService.stopTask(sessionId as SessionID);

    if (result.success) {
      return {
        success: true,
        partialResult: {
          taskId: taskId || 'unknown',
          status: 'cancelled',
        },
      };
    }

    return result;
  }

  // ============================================================
  // Token Accounting (NEW)
  // ============================================================

  /**
   * Normalize Codex SDK response to common format
   *
   * Codex doesn't support caching, so cache tokens are always 0.
   */
  normalizedSdkResponse(rawResponse: RawSdkResponse): NormalizedSdkResponse {
    if (rawResponse.tool !== 'codex') {
      throw new Error(`Expected codex response, got ${rawResponse.tool}`);
    }

    const codexResponse = rawResponse as CodexSdkResponse;

    // Extract token usage with defaults
    const tokenUsage = codexResponse.tokenUsage || {
      input_tokens: 0,
      output_tokens: 0,
      total_tokens: 0,
    };

    return {
      userMessageId: codexResponse.userMessageId,
      assistantMessageIds: codexResponse.assistantMessageIds,
      tokenUsage: {
        inputTokens: tokenUsage.input_tokens || 0,
        outputTokens: tokenUsage.output_tokens || 0,
        totalTokens: tokenUsage.total_tokens || tokenUsage.input_tokens! + tokenUsage.output_tokens! || 0,
        cacheReadTokens: 0, // Codex doesn't support caching
        cacheCreationTokens: 0, // Codex doesn't support caching
      },
      contextWindow: codexResponse.contextWindow,
      contextWindowLimit: codexResponse.contextWindowLimit,
      model: codexResponse.model,
      durationMs: codexResponse.durationMs,
    };
  }

}
